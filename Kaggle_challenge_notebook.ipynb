{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":63982,"databundleVersionId":7009041,"sourceType":"competition"}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\n\n#code pour recup les raw data\n\n# LOAD DES DONNEES\n\n# Fonction pour charger les données à partir d'un dossier\ndef load_data(folder):\n    data = []\n    \n    # Parcourez tous les fichiers CSV dans le dossier principal\n    for label, file in enumerate(os.listdir(folder)):\n        file_path = os.path.join(folder, file)\n        \n        # Vérifiez si l'élément est un fichier CSV\n        if file.endswith(\".csv\") and os.path.isfile(file_path):\n            df = pd.read_csv(file_path)\n            \n            # Aplatir les données et les ajouter à la liste\n            data.append(df.values.flatten())  \n    for i in range(len(data)) :    # Séparer les éléments des sous-listes en sous-listes pour que test_data[i][j][k] corresponde au ième fichier, jème ligne et un sélectionne x, y, polarity ou time avec k. \n        data[i] = np.array(data[i]).reshape(-1, 4).tolist()\n    return data\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-08T18:03:20.410253Z","iopub.execute_input":"2023-12-08T18:03:20.411093Z","iopub.status.idle":"2023-12-08T18:03:20.418746Z","shell.execute_reply.started":"2023-12-08T18:03:20.411056Z","shell.execute_reply":"2023-12-08T18:03:20.417751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_data_dictionary(folder_path):\n    dict_data = {}\n\n    for i, subfolder in enumerate(os.listdir(folder_path)):\n        subfolder_path = os.path.join(folder_path, subfolder)\n        data_list = load_data(subfolder_path)\n        dict_data[str(i)] = data_list\n\n    return dict_data\n\n# Example usage\nfolder_path = '/kaggle/input/smemi309-final-evaluation-challenge-2023/train10/train10'\ndata_dict = create_data_dictionary(folder_path)\n\n# Accessing values in the data_dict\nfor label, data_list in data_dict.items():\n    print(f\"Label: {label}, Data List Length: {len(data_list)}\")\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:03:26.712948Z","iopub.execute_input":"2023-12-08T18:03:26.713329Z","iopub.status.idle":"2023-12-08T18:06:06.527994Z","shell.execute_reply.started":"2023-12-08T18:03:26.713298Z","shell.execute_reply":"2023-12-08T18:06:06.526900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage\nfolder_path = '/kaggle/input/smemi309-final-evaluation-challenge-2023/test10'\ndata_dict_test = create_data_dictionary(folder_path)\n\n# Accessing values in the data_dict\nfor label, data_list in data_dict_test.items():\n    print(f\"Label: {label}, Data List Length: {len(data_list)}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:06:06.529669Z","iopub.execute_input":"2023-12-08T18:06:06.530046Z","iopub.status.idle":"2023-12-08T18:06:47.020892Z","shell.execute_reply.started":"2023-12-08T18:06:06.530016Z","shell.execute_reply":"2023-12-08T18:06:47.019770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preprocessing des données","metadata":{}},{"cell_type":"code","source":"def interv(data,n): # Sépare le dataframe en n intervalle de temps\n    Temps = []\n    L = len(data)\n    for i in range(n):\n        Temps.append(i * len(data) // n)\n    Temps.append(L-1)\n    return Temps\n\ndef separedata(data,n):\n    D = []\n    S = interv(data,n)\n    for i in range(1,len(S)):\n        D.append(data[S[i-1]:S[i]])\n    return D","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:06:47.022139Z","iopub.execute_input":"2023-12-08T18:06:47.022441Z","iopub.status.idle":"2023-12-08T18:06:47.029267Z","shell.execute_reply.started":"2023-12-08T18:06:47.022414Z","shell.execute_reply":"2023-12-08T18:06:47.028321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mise sous form matricielle des données","metadata":{}},{"cell_type":"code","source":"def matr(data):   #n = nombre d'intervalle    (à faire avec data = test_data[0])\n    list_data = []\n    for j in range(len(data)):\n        list_data.append([data[j][0], data[j][1], data[j][2]])      #[[1,2,3],[4,5,6],[7,8,9]]\n    M = np.zeros([224,90])\n    for i in range(len(data)):\n            if list_data[i][2] == 1 :\n                M[list_data[i][0],list_data[i][1]] += 1   #M[x,y] = somme des polarités\n            else :\n                M[list_data[i][0],list_data[i][1]] -= 1\n    return M","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:06:47.031734Z","iopub.execute_input":"2023-12-08T18:06:47.032050Z","iopub.status.idle":"2023-12-08T18:06:47.043666Z","shell.execute_reply.started":"2023-12-08T18:06:47.032023Z","shell.execute_reply":"2023-12-08T18:06:47.042723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convolution des matrices","metadata":{}},{"cell_type":"code","source":"def sum_by_block(M,Lx,Ly):\n\n    # Calculer les dimensions de la nouvelle matrice B\n    nb_lignes_B = M.shape[0] // Lx\n    nb_colonnes_B = M.shape[1] // Ly\n\n    # Initialiser la nouvelle matrice B avec des zéros\n    B = np.zeros((nb_lignes_B, nb_colonnes_B), dtype=M.dtype)\n\n    # Parcourir les blocs de la matrice M\n    for i in range(nb_lignes_B):\n        for j in range(nb_colonnes_B):\n            # Extraire le bloc correspondant de la matrice M avec les longueurs Lx et Ly\n            bloc_M = M[i * Lx:(i + 1) * Lx, j * Ly:(j + 1) * Ly]\n\n            # Ajouter la somme des éléments du bloc à la matrice B\n            B[i, j] = np.sum(bloc_M)\n\n    # Sors la matrice résultante B\n    return B\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:06:47.044746Z","iopub.execute_input":"2023-12-08T18:06:47.045189Z","iopub.status.idle":"2023-12-08T18:06:47.054843Z","shell.execute_reply.started":"2023-12-08T18:06:47.045159Z","shell.execute_reply":"2023-12-08T18:06:47.053902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def label_bloc(data, n, Lx, Ly) :\n    M = []\n    for i in range(32) :\n        M.append([])\n        for j in range(n) :\n            M[i].append(sum_by_block(matr(separedata(data[i],n)[j]), Lx, Ly))\n    return M","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:06:47.055996Z","iopub.execute_input":"2023-12-08T18:06:47.057688Z","iopub.status.idle":"2023-12-08T18:06:47.065086Z","shell.execute_reply.started":"2023-12-08T18:06:47.057661Z","shell.execute_reply":"2023-12-08T18:06:47.064230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def label_bloc_test(data, n, Lx, Ly) :\n    M = []\n    for i in range(100) :\n        M.append([])\n        for j in range(n) :\n            M[i].append(sum_by_block(matr(separedata(data[i],n)[j]), Lx, Ly))\n    return M","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:06:47.066292Z","iopub.execute_input":"2023-12-08T18:06:47.066944Z","iopub.status.idle":"2023-12-08T18:06:47.075376Z","shell.execute_reply.started":"2023-12-08T18:06:47.066918Z","shell.execute_reply":"2023-12-08T18:06:47.074503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = 20 # Nombre d'intervalles\nLx = 28 # Longueur bloc\nLy = 15 # Largeur bloc\n\ndict_label_bloc = {}\n\nfor key in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n    dict_label_bloc[key] = label_bloc(data_dict[key], n, Lx, Ly)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:06:47.076573Z","iopub.execute_input":"2023-12-08T18:06:47.076882Z","iopub.status.idle":"2023-12-08T18:10:43.879042Z","shell.execute_reply.started":"2023-12-08T18:06:47.076836Z","shell.execute_reply":"2023-12-08T18:10:43.878053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = 20 # Nombre d'intervalles\nLx = 28 # Longueur bloc\nLy = 15 # Largeur bloc\n\ndict_label_bloc_test = {}\ndict_label_bloc_test['0'] = label_bloc_test(data_dict_test['0'], n, Lx, Ly)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:10:43.880284Z","iopub.execute_input":"2023-12-08T18:10:43.880611Z","iopub.status.idle":"2023-12-08T18:11:51.306469Z","shell.execute_reply.started":"2023-12-08T18:10:43.880582Z","shell.execute_reply":"2023-12-08T18:11:51.305593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"POUR LE TRAIN\n","metadata":{}},{"cell_type":"code","source":"data_flat = [element for sous_liste in dict_label_bloc.values() for element in sous_liste]\nprint(np.shape(data_flat))\ndata_flat=np.array(data_flat)\ndata_flat2=data_flat.reshape(320,-1)\nprint(np.shape(data_flat2))","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:11:51.309740Z","iopub.execute_input":"2023-12-08T18:11:51.310486Z","iopub.status.idle":"2023-12-08T18:11:51.327128Z","shell.execute_reply.started":"2023-12-08T18:11:51.310431Z","shell.execute_reply":"2023-12-08T18:11:51.326243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"POUR LE TEST\n","metadata":{}},{"cell_type":"code","source":"data_flat_test = [element for sous_liste in dict_label_bloc_test.values() for element in sous_liste]\nprint(np.shape(data_flat_test))\ndata_flat_test=np.array(data_flat_test)\ndata_flat2_test=data_flat_test.reshape(100,-1)\nprint(np.shape(data_flat2_test))","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:11:51.328191Z","iopub.execute_input":"2023-12-08T18:11:51.328559Z","iopub.status.idle":"2023-12-08T18:11:51.339158Z","shell.execute_reply.started":"2023-12-08T18:11:51.328528Z","shell.execute_reply":"2023-12-08T18:11:51.338124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RANDOM FOREST CLASSFIER V1\n#Tester pour diffrentes valeurs de n, Lx,Ly et n_estimator\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Chargez les données du dictionnaire\ndef randomforest(dictionnaire,n_est):\n    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n    data_flat=np.array(data_flat)\n    data_flat2=data_flat.reshape(320,-1)\n\n    lab=np.array([[0]*32,[1]*32,[2]*32,[3]*32,[4]*32,[5]*32,[6]*32,[7]*32,[8]*32,[9]*32])\n    lab = [element for sous_liste in lab for element in sous_liste]\n\n    for i in range(len(lab)):\n        lab[i]=np.array(lab[i])\n    # Séparez les données en ensemble de test et d'apprentissage\n\n    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.4, random_state=7)\n\n    # Créez un modèle Random Forest fair varier le n__estimator\n    clf = RandomForestClassifier(n_estimators=n_est)\n\n    # Entraînez le modèle sur l'ensemble d'apprentissage\n    clf.fit(X_train, y_train)\n\n    # Prédisez les sorties pour l'ensemble de test\n    y_pred = clf.predict(X_test)\n\n    # Calculez la précision du modèle\n    print(\"Précision :\", accuracy_score(y_test, y_pred))\n    return accuracy_score(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RANDOM FOREST CLASSFIER\n#Tester pour diffrentes valeurs de n, Lx,Ly et n_estimator\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ndef RandomForest(dictionnaire, Nb_estim, Random):\n    # Chargez les données du dictionnaire\n    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n    data_flat=np.array(data_flat)\n    data_flat2=data_flat.reshape(320,-1)\n\n    lab=np.array([[0]*32,[1]*32,[2]*32,[3]*32,[4]*32,[5]*32,[6]*32,[7]*32,[8]*32,[9]*32])\n    lab = [element for sous_liste in lab for element in sous_liste]\n\n    for i in range(len(lab)):\n        lab[i]=np.array(lab[i])\n    # Séparez les données en ensemble de test et d'apprentissage\n\n    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=Random)\n\n    # Créez un modèle Random Forest fair varier le n__estimator\n    clf = RandomForestClassifier(n_estimators=Nb_estim)\n\n    # Entraînez le modèle sur l'ensemble d'apprentissage\n    clf.fit(X_train, y_train)\n\n    # Prédisez les sorties pour l'ensemble de test\n    y_pred = clf.predict(X_test)\n\n    # Calculez la précision du modèle\n    \"\"\"print(\"Précision :\", accuracy_score(y_test, y_pred))\"\"\"\n    return accuracy_score(y_test, y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gridsearch de parametre optimal","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\ndef RandomForestGridSearch(dictionnaire, Random_state):\n    # Chargez les données du dictionnaire\n    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n    data_flat = np.array(data_flat)\n    data_flat2 = data_flat.reshape(320, -1)\n\n    lab = np.array([[0] * 32, [1] * 32, [2] * 32, [3] * 32, [4] * 32, [5] * 32, [6] * 32, [7] * 32, [8] * 32, [9] * 32])\n    lab = [element for sous_liste in lab for element in sous_liste]\n\n    for i in range(len(lab)):\n        lab[i] = np.array(lab[i])\n\n    # Séparez les données en ensemble de test et d'apprentissage\n    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=random_state)\n\n    # Paramètres à tester\n    param_grid = {'n_estimators': np.arange(600,1000,50)}\n\n    # Créez un modèle Random Forest\n    clf = RandomForestClassifier()\n\n    # Créez un objet GridSearchCV\n    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n\n    # Entraînez le modèle sur l'ensemble d'apprentissage\n    grid_search.fit(X_train, y_train)\n\n    # Meilleurs paramètres\n    best_params = grid_search.best_params_\n\n    # Utilisez le meilleur modèle pour prédire les sorties pour l'ensemble de test\n    y_pred = grid_search.predict(X_test)\n\n    # Calculez la précision du modèle avec les meilleurs paramètres\n    best_accuracy = accuracy_score(y_test, y_pred)\n\n    return best_params, best_accuracy\n\n# Exemple d'utilisation\nrandom_state = 42  # Vous pouvez choisir n'importe quelle valeur pour la reproductibilité\nbest_params, best_accuracy = RandomForestGridSearch(dict_label_bloc, 7)\n\nprint(\"Meilleurs paramètres:\", best_params)\nprint(\"Précision avec meilleurs paramètres:\", best_accuracy)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Graphe","metadata":{}},{"cell_type":"code","source":"# Graphe RandomForest en fonction du nombre d'estimateurs\n\nimport matplotlib.pyplot as plt\n\nN = np.arange(25,1200,25)\nRF = []\nfor i in range(len(N)):\n    moy = []\n    for j in range(10):\n        moy.append(RandomForest(dict_label_bloc, N[i], i)) # random_state = j\n    moy = np.mean(moy)\n    RF.append(moy)\n\nplt.plot(N, RF, color = 'green', marker = 'o', label = 'Random Forest')\nplt.legend()\n\nprint('Précision maximum :', np.max(RF), ' atteint pour', N[np.argmax(RF)],' estimateurs')\nprint('Variance de la précision RandomForest :', np.var(RF))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"random forest avec adaboosting","metadata":{}},{"cell_type":"code","source":"# Random Forest avec AdaBoost\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ndef RandomForestAdaBoost(dictionnaire, Nb_estimRF, Nb_estimAB, Random_state):\n    # Charger les données du dictionnaire\n    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n    data_flat = np.array(data_flat)\n    data_flat2 = data_flat.reshape(320, -1)\n\n    lab = np.array([[0] * 32, [1] * 32, [2] * 32, [3] * 32, [4] * 32, [5] * 32, [6] * 32, [7] * 32, [8] * 32, [9] * 32])\n    lab = [element for sous_liste in lab for element in sous_liste]\n\n    for i in range(len(lab)):\n        lab[i] = np.array(lab[i])\n\n    # Séparer les données en ensemble de test et d'apprentissage\n    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.2, random_state=Random_state)\n\n    # Créer un modèle Random Forest en faisant varier le n_estimators\n    clf_rf = RandomForestClassifier(n_estimators=Nb_estimRF)\n    clf_rf.fit(X_train, y_train)\n\n    # Prédire les sorties pour l'ensemble de test avec Random Forest\n    y_pred_rf = clf_rf.predict(X_test)\n\n    # Calculer la précision du modèle Random Forest\n    \"\"\"print(\"Précision Random Forest:\", accuracy_score(y_test, y_pred_rf))\"\"\"\n\n    # Créer un modèle AdaBoost\n    clf_ab = AdaBoostClassifier(n_estimators=Nb_estimAB, base_estimator=RandomForestClassifier(n_estimators=Nb_estimRF))\n\n    # Entraîner le modèle AdaBoost sur l'ensemble d'apprentissage\n    clf_ab.fit(X_train, y_train)\n\n    # Prédire les sorties pour l'ensemble de test avec AdaBoost\n    y_pred_ab = clf_ab.predict(X_test)\n\n    # Calculer la précision du modèle AdaBoost\n    \"\"\"print(\"Précision AdaBoost:\", accuracy_score(y_test, y_pred_ab))\"\"\"\n   \n    return accuracy_score(y_test, y_pred_rf), accuracy_score(y_test, y_pred_ab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Graphe avec adaboosted random forest et sans ","metadata":{}},{"cell_type":"code","source":"# Graphe RandomForest avec et sans AdaBoost\n\nimport matplotlib.pyplot as plt\n\nN = np.arange(25,1200,25)\nRF = []\nAB = []\nfor i in range(len(N)):\n    moyRF = []\n    moyAB = []\n    for j in range(10):\n        RFAB = RandomForestAdaBoost(dict_label_bloc, 825, N[i], j) #825 meilleur paramètre pour RF\n        moyRF.append(RFAB[0]) # RandomForest\n        moyAB.append(RFAB[1]) # AdaBoost\n    moyRF = np.mean(moyRF)\n    moyAB = np.mean(moyAB)\n    RF.append(moyRF)\n    AB.append(moyAB)\n\nplt.plot(N, RF, color = 'green', marker = 'o', label = 'Random Forest')\nplt.plot(N, AB, color = 'red', marker = 'o', label = 'AdaBoost')\nplt.legend()\n\nprint('Précision maximum RandomForest avec AdaBoost :', np.max(AB), ' atteint pour', N[np.argmax(AB)],' estimateurs que prend AdaBoost')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GradientBoosting and Xtreme gradient boosting of randomforest","metadata":{}},{"cell_type":"code","source":"# Random Forest avec Extrem Grandient Boosting\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ndef RandomForestGradientBoosting(dictionnaire, Nb_estimRF, Nb_estimGB):\n    # Charger les données du dictionnaire\n    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n    data_flat = np.array(data_flat)\n    data_flat2 = data_flat.reshape(320, -1)\n\n    lab = np.array([[0] * 32, [1] * 32, [2] * 32, [3] * 32, [4] * 32, [5] * 32, [6] * 32, [7] * 32, [8] * 32, [9] * 32])\n    lab = [element for sous_liste in lab for element in sous_liste]\n\n    for i in range(len(lab)):\n        lab[i] = np.array(lab[i])\n\n    # Séparer les données en ensemble de test et d'apprentissage\n    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=7)\n\n    # Créer un modèle Random Forest en faisant varier le n_estimators\n    clf_rf = RandomForestClassifier(n_estimators=Nb_estimRF)\n    clf_rf.fit(X_train, y_train)\n\n    # Prédire les sorties pour l'ensemble de test avec Random Forest\n    y_pred_rf = clf_rf.predict(X_test)\n\n    # Calculer la précision du modèle Random Forest\n    print(\"Précision Random Forest:\", accuracy_score(y_test, y_pred_rf))\n\n    # Créer un modèle Gradient Boosting\n    clf_gb = GradientBoostingClassifier(n_estimators=Nb_estimGB, learning_rate=0.1, max_depth=3)\n\n    # Entraîner le modèle sur l'ensemble d'apprentissage\n    clf_gb.fit(X_train, y_train)\n\n    # Prédire les sorties pour l'ensemble de test avec Gradient Boosting\n    y_pred_gb = clf_gb.predict(X_test)\n\n    # Calculer la précision du modèle Gradient Boosting\n    print(\"Précision Gradient Boosting:\", accuracy_score(y_test, y_pred_gb))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest avec Extrem Grandient Boosting\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ndef RandomForestGradientBoosting(dictionnaire, Nb_estimRF, Nb_estimGB):\n    # Charger les données du dictionnaire\n    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n    data_flat = np.array(data_flat)\n    data_flat2 = data_flat.reshape(320, -1)\n\n    lab = np.array([[0] * 32, [1] * 32, [2] * 32, [3] * 32, [4] * 32, [5] * 32, [6] * 32, [7] * 32, [8] * 32, [9] * 32])\n    lab = [element for sous_liste in lab for element in sous_liste]\n\n    for i in range(len(lab)):\n        lab[i] = np.array(lab[i])\n\n    # Séparer les données en ensemble de test et d'apprentissage\n    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=7)\n\n    # Créer un modèle Random Forest en faisant varier le n_estimators\n    clf_rf = RandomForestClassifier(n_estimators=Nb_estimRF)\n    clf_rf.fit(X_train, y_train)\n\n    # Prédire les sorties pour l'ensemble de test avec Random Forest\n    y_pred_rf = clf_rf.predict(X_test)\n\n    # Calculer la précision du modèle Random Forest\n    print(\"Précision Random Forest:\", accuracy_score(y_test, y_pred_rf))\n\n    # Créer un modèle Gradient Boosting\n    clf_gb = GradientBoostingClassifier(n_estimators=Nb_estimGB, learning_rate=0.1, max_depth=3)\n\n    # Entraîner le modèle sur l'ensemble d'apprentissage\n    clf_gb.fit(X_train, y_train)\n\n    # Prédire les sorties pour l'ensemble de test avec Gradient Boosting\n    y_pred_gb = clf_gb.predict(X_test)\n\n    # Calculer la précision du modèle Gradient Boosting\n    print(\"Précision Gradient Boosting:\", accuracy_score(y_test, y_pred_gb))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Extra trees and graph","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\ndef ExtraTrees(dictionnaire, Nb_estim, Random):\n    # Chargez les données du dictionnaire\n    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n    data_flat = np.array(data_flat)\n    data_flat2 = data_flat.reshape(320, -1)\n\n    lab = np.array([[0] * 32, [1] * 32, [2] * 32, [3] * 32, [4] * 32, [5] * 32, [6] * 32, [7] * 32, [8] * 32, [9] * 32])\n    lab = [element for sous_liste in lab for element in sous_liste]\n\n    for i in range(len(lab)):\n        lab[i] = np.array(lab[i])\n\n    # Séparez les données en ensemble de test et d'apprentissage\n    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=Random)\n\n    # Créez un modèle Extra Trees Classifier en faisant varier le n__estimator\n    clf = ExtraTreesClassifier(n_estimators=Nb_estim)\n\n    # Entraînez le modèle sur l'ensemble d'apprentissage\n    clf.fit(X_train, y_train)\n\n    # Prédisez les sorties pour l'ensemble de test\n    y_pred = clf.predict(X_test)\n\n    # Calculez la précision du modèle\n    \"\"\"print(\"Précision :\", accuracy_score(y_test, y_pred))\"\"\"\n    return accuracy_score(y_test, y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Graphe Extra Trees en fonction du nombre d'estimateurs\n\nimport matplotlib.pyplot as plt\n\nN = np.arange(50,1000,50)\nET = []\nfor i in range(len(N)):\n    moy = []\n    for j in range(10):\n        moy.append(ExtraTrees(dict_label_bloc, N[i], j)) # random_state = j\n    moy = np.mean(moy)\n    ET.append(moy)\n\nplt.plot(N, ET, color = 'green', marker = 'o', label = 'Extra Trees')\nplt.legend()\n\nprint('Précision maximum :', np.max(ET), ' atteint pour', N[np.argmax(ET)],' estimateurs')\nprint('Variance de la précision RandomForest :', np.var(ET))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"randomforest(dict_label_bloc,650)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest avec AdaBoost et bagging\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ndef Rf_Ab_Bg(dictionnaire, Nb_estimRF, Nb_estimAB,Nb_estimBG):\n    # Charger les données du dictionnaire\n    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n    data_flat = np.array(data_flat)\n    data_flat2 = data_flat.reshape(320, -1)\n\n    lab = np.array([[0] * 32, [1] * 32, [2] * 32, [3] * 32, [4] * 32, [5] * 32, [6] * 32, [7] * 32, [8] * 32, [9] * 32])\n    lab = [element for sous_liste in lab for element in sous_liste]\n\n    for i in range(len(lab)):\n        lab[i] = np.array(lab[i])\n\n    # Séparer les données en ensemble de test et d'apprentissage\n    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=7)\n\n    ###RANDOM FOREST\n    # Créer un modèle Random Forest en faisant varier le n_estimators\n    clf_rf = RandomForestClassifier(n_estimators=Nb_estimRF)\n    clf_rf.fit(X_train, y_train)\n\n    # Prédire les sorties pour l'ensemble de test avec Random Forest\n    y_pred_rf = clf_rf.predict(X_test)\n\n    # Calculer la précision du modèle Random Forest\n    print(\"Précision Random Forest:\", accuracy_score(y_test, y_pred_rf))\n    \n    ###ADABOOST\n    # Créer un modèle AdaBoost\n    clf_ab = AdaBoostClassifier(n_estimators=Nb_estimAB, base_estimator=RandomForestClassifier(n_estimators=Nb_estimRF))\n\n    # Entraîner le modèle AdaBoost sur l'ensemble d'apprentissage\n    clf_ab.fit(X_train, y_train)\n\n    # Prédire les sorties pour l'ensemble de test avec AdaBoost\n    y_pred_ab = clf_ab.predict(X_test)\n\n    # Calculer la précision du modèle AdaBoost\n    print(\"Précision AdaBoost:\", accuracy_score(y_test, y_pred_ab))\n    \n    ###BAGGING\n    ###ADABOOST\n    # Créer un modèle Bagging\n    clf_bg = BaggingClassifier(n_estimators=Nb_estimBG, base_estimator=RandomForestClassifier(n_estimators=Nb_estimRF))\n\n    # Entraîner le modèle AdaBoost sur l'ensemble d'apprentissage\n    clf_bg.fit(X_train, y_train)\n\n    # Prédire les sorties pour l'ensemble de test avec AdaBoost\n    y_pred_bg = clf_ab.predict(X_test)\n\n    # Calculer la précision du modèle AdaBoost\n    print(\"Précision Bagging:\", accuracy_score(y_test, y_pred_ab))\n    \n    return(accuracy_score(y_test, y_pred_rf),accuracy_score(y_test, y_pred_ab),accuracy_score(y_test, y_pred_bg))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:35:37.318625Z","iopub.execute_input":"2023-12-08T18:35:37.319165Z","iopub.status.idle":"2023-12-08T18:35:37.332501Z","shell.execute_reply.started":"2023-12-08T18:35:37.319116Z","shell.execute_reply":"2023-12-08T18:35:37.331647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nRf_Ab_Bg(dict_label_bloc, 650, 150,50)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nY=[]\nfor i in range(10):\n    Y.append(Rf_Ab_Bg(dict_label_bloc, 650, 150,40))\nZ=np.array(Y)\nprint(np.shape(Z))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:35:45.170063Z","iopub.execute_input":"2023-12-08T18:35:45.170466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Z)\n\nprint(Z[:,0])\nprint(Z[:,1])\nprint(Z[:,2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming Z has three columns corresponding to Random Forest, AdaBoost, and Bagging\nx = np.arange(0, 10, 1)\n\n# Création de la figure\nplt.figure()\n\n# Tracé des courbes avec légendes\nplt.plot(x, Z[:, 0], color='green', label='Random Forest')\nplt.plot(x, Z[:, 1], color='orange', label='AdaBoost')\nplt.plot(x, Z[:, 2], color='blue', label='Bagging')\n\n# Ajout de la légende\nplt.legend()\n\n# Ajout des titres et étiquettes d'axe\nplt.title(\"Comparison of AdaBoost, Bagging, and Random Forest over multiple classifications\")\nplt.xlabel('Classification')\nplt.ylabel('Accuracy score')\n\n# Affichage du graphique\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obersvation de la pertinence d'appliquer un Boosting ou un Bagging (ci dessus)","metadata":{}},{"cell_type":"code","source":"def calculate_weights(X_test, y_test, classifiers):\n    weights = {}\n    for name, classifier in classifiers:\n        y_pred_single = classifier.predict(X_test)\n        accuracy_single = accuracy_score(y_test, y_pred_single)\n        weights[name] = accuracy_single\n    total_weight = sum(weights.values())\n    return {name: weight / total_weight for name, weight in weights.items()}\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:11:51.340261Z","iopub.execute_input":"2023-12-08T18:11:51.340848Z","iopub.status.idle":"2023-12-08T18:11:51.347110Z","shell.execute_reply.started":"2023-12-08T18:11:51.340813Z","shell.execute_reply":"2023-12-08T18:11:51.346044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#VOTING WITH DIFFERENT CLASSIFICATIONS\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ndef voting(dictionnaire, classifiers):\n    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n    data_flat = np.array(data_flat)\n    data_flat2 = data_flat.reshape(320, -1)\n\n    lab = np.array([[0] * 32, [1] * 32, [2] * 32, [3] * 32, [4] * 32, [5] * 32, [6] * 32, [7] * 32, [8] * 32, [9] * 32])\n    lab = [element for sous_liste in lab for element in sous_liste]\n\n    for i in range(len(lab)):\n        lab[i] = np.array(lab[i])\n\n    # Séparer les données en ensemble de test et d'apprentissage\n    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=7)\n\n    # Create a VotingClassifier with various types of classifiers\n    voting_classifier = VotingClassifier(estimators=classifiers, voting='soft')  # Use 'hard' for hard voting\n\n    # Train each individual classifier before training the ensemble\n    for name, classifier in classifiers:\n        classifier.fit(X_train, y_train)\n\n    # Train the ensemble classifier\n    voting_classifier.fit(X_train, y_train)\n\n    # Make predictions\n    y_pred = voting_classifier.predict(X_test)\n\n    # Evaluate accuracy for each classifier\n    accuracies = {}\n    for name, classifier in classifiers:\n        y_pred_single = classifier.predict(X_test)\n        accuracy_single = accuracy_score(y_test, y_pred_single)\n        accuracies[name] = accuracy_single\n        print(f'{name} Accuracy: {accuracy_single:.2f}')\n\n    # Evaluate overall ensemble accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f'Ensemble Accuracy: {accuracy:.2f}')\n\n    return accuracies, accuracy\n\n# Example usage\nclassifiers = [\n    ('rf', RandomForestClassifier(n_estimators=440, random_state=1)),\n    ('ada', AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=440, random_state=1),n_estimators=10, random_state=2)),\n    ('svc', SVC(probability=True, random_state=3)),\n    ('dt', DecisionTreeClassifier(random_state=5)),\n    ('lr', LogisticRegression(random_state=4)),\n    ('knn', KNeighborsClassifier(n_neighbors=2)),\n    ('nb', GaussianNB()),\n    ('mlp', MLPClassifier(random_state=6)),\n    ('bagging', BaggingClassifier(base_estimator=RandomForestClassifier(n_estimators=440, random_state=1), n_estimators=40, random_state=7)),\n    ('extra_trees', ExtraTreesClassifier(n_estimators=100, random_state=9)),\n]\n\n# Call the function with the classifiers\nclassifier_accuracies, ensemble_accuracy = voting(dict_label_bloc, classifiers)\n\n# Access individual classifier accuracies\nprint(\"\\nIndividual Classifier Accuracies:\")\nfor name, accuracy in classifier_accuracies.items():\n    print(f'{name}: {accuracy:.2f}')\n\n# Access overall ensemble accuracy\nprint(f'\\nOverall Ensemble Accuracy: {ensemble_accuracy:.2f}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On eneleve les classifier donnant des scores inférieurs à 40%. Et on ajoute une notion de poids concernant les votes proportionnels à l'accuracy du classifieur","metadata":{}},{"cell_type":"markdown","source":"Let's try with only either Adaboost, rf or bagging because the classification is redundant","metadata":{}},{"cell_type":"markdown","source":"We have very similar results with way better speed","metadata":{}},{"cell_type":"code","source":"#VOTING WITH DIFFERENT CLASSIFICATIONS\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ndef voting(dictionnaire, classifiers):\n    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n    data_flat = np.array(data_flat)\n    data_flat2 = data_flat.reshape(320, -1)\n\n    lab = np.array([[0] * 32, [1] * 32, [2] * 32, [3] * 32, [4] * 32, [5] * 32, [6] * 32, [7] * 32, [8] * 32, [9] * 32])\n    lab = [element for sous_liste in lab for element in sous_liste]\n\n    for i in range(len(lab)):\n        lab[i] = np.array(lab[i])\n\n    # Séparer les données en ensemble de test et d'apprentissage\n    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=7)\n\n    # Create a VotingClassifier with various types of classifiers\n    voting_classifier = VotingClassifier(estimators=classifiers, voting='soft')  # Use 'hard' for hard voting\n\n    # Train each individual classifier before training the ensemble\n    for name, classifier in classifiers:\n        classifier.fit(X_train, y_train)\n\n    # Train the ensemble classifier\n    voting_classifier.fit(X_train, y_train)\n\n    # Make predictions\n    y_pred = voting_classifier.predict(X_test)\n\n    # Evaluate accuracy for each classifier\n    accuracies = {}\n    for name, classifier in classifiers:\n        y_pred_single = classifier.predict(X_test)\n        accuracy_single = accuracy_score(y_test, y_pred_single)\n        accuracies[name] = accuracy_single\n        print(f'{name} Accuracy: {accuracy_single:.2f}')\n\n    # Evaluate overall ensemble accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f'Ensemble Accuracy: {accuracy:.2f}')\n    for name, classifier in classifiers:\n        classifier.fit(X_train, y_train)\n\n    # Calculate weights based on accuracy\n    classifier_weights = calculate_weights(X_test, y_test, classifiers)\n    print(classifier_weights)\n    return accuracies, accuracy\n    voting_classifier = VotingClassifier(estimators=classifiers, voting='soft', weights=list(classifier_weights.values()))\n\n# Example usage\nclassifiers = [\n    \n    ('ABrf1', AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=825, random_state=4), n_estimators=600, random_state=7)),\n    ('ABrf2', AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=825, random_state=6), n_estimators=600, random_state=8)),\n    ('ABrf3', AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=825, random_state=4), n_estimators=600, random_state=9)),\n    ]\n\n# Call the function with the classifiers\nclassifier_accuracies, ensemble_accuracy = voting(dict_label_bloc, classifiers)\n\n# Access individual classifier accuracies\nprint(\"\\nIndividual Classifier Accuracies:\")\nfor name, accuracy in classifier_accuracies.items():\n    print(f'{name}: {accuracy:.2f}')\n\n# Access overall ensemble accuracy\nprint(f'\\nOverall Ensemble Accuracy: {ensemble_accuracy:.2f}')\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    ('extra_trees1', ExtraTreesClassifier(n_estimators=550, random_state=1)),\n    ('extra_trees2', ExtraTreesClassifier(n_estimators=550, random_state=3)),\n    ('extra_trees3', ExtraTreesClassifier(n_estimators=550, random_state=5))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Attempt at RNN classification","metadata":{}},{"cell_type":"code","source":"data_flat = [element for sous_liste in dict_label_bloc.values() for element in sous_liste]\ndata_flat = np.array(data_flat)\n\n# Assuming original data shape is (320, 20, 8, 6)\nnum_samples, time_steps, x_dim, y_dim = data_flat.shape\n\n# Flatten along x and y axes\ndata_flat2 = data_flat.reshape((num_samples, time_steps, -1))\n\n# data_flat2 now has shape (320, 20, 8*6)\n\n#shape transformation to fit an RNN\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.utils import to_categorical\n\n# Assuming you have your data in X (features) and y (labels)\n\ndata_flat = [element for sous_liste in dict_label_bloc.values() for element in sous_liste]\ndata_flat = np.array(data_flat)\n# Assuming original data shape is (320, 20, 8, 6)\nnum_samples, time_steps, x_dim, y_dim = data_flat.shape\n\n# Flatten along x and y axes, keeping the time axis intact\ndata_flat2 = data_flat.reshape((num_samples, time_steps, -1))  \n\n# Convert labels to one-hot encoding\ny_one_hot = to_categorical(y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(data_flat2, y_one_hot, test_size=0.25, random_state=42)\n\n# Build a simple RNN model for categorical classification\nrnn_model = models.Sequential([\n    layers.SimpleRNN(64, activation='relu', input_shape=(time_steps, x_dim * y_dim)),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')  # 10 classes for categorical classification\n])\n\nrnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the RNN model\nrnn_model.fit(X_train, y_train, epochs=25, batch_size=32, validation_split=0.2)\n\n# Evaluate RNN model on test set\ny_pred_rnn = rnn_model.predict(X_test)\ny_pred_rnn_classes = tf.argmax(y_pred_rnn, axis=1)\ny_test_classes = tf.argmax(y_test, axis=1)\n\naccuracy_rnn = accuracy_score(y_test_classes, y_pred_rnn_classes)\nprint(f'RNN Accuracy: {accuracy_rnn:.2f}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\n#RANDOM FOREST BOOSTED WITH TREE ON VOTING CLASSFIER FOR TEST DATA WITH ALL TRAINING DATA\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\n\n# Chargez les données du dictionnaire\ndata_flat = [element for sous_liste in dict_label_bloc.values() for element in sous_liste]\ndata_flat=np.array(data_flat)\ndata_flat2=data_flat.reshape(320,-1)\n\nlab=np.array([[0]*32,[1]*32,[2]*32,[3]*32,[4]*32,[5]*32,[6]*32,[7]*32,[8]*32,[9]*32])\nlab = [element for sous_liste in lab for element in sous_liste]\n\nfor i in range(len(lab)):\n    lab[i]=np.array(lab[i])\n# Séparez les données en ensemble de test et d'apprentissage\n\nX_train2 = data_flat2\ny_train2 = lab\n#les data pour la classification du voting\n\n# Séparer les données en ensemble de test et d'apprentissage\nX_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=7)\n#les data pour les classifications dans le boosting pour avoir une accuracy\n\n#Classifiers :    \nclassifiers = []\n\n# Assuming you want 10 sets of AdaBoost with RandomForest and 10 sets of ExtraTrees classifiers\nnum_classifiers = 5\n\nfor i in range(1, num_classifiers + 1):\n    ab_rf_classifier = AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=825, random_state=i), n_estimators=300, random_state=i + 10)\n    extra_trees_classifier = ExtraTreesClassifier(n_estimators=800, random_state=i + 20)\n    classifiers.append(('AB_rf_{}'.format(i), ab_rf_classifier))\n    classifiers.append(('extra_trees_{}'.format(i), extra_trees_classifier))\n    \n#Creation of voting classifier :\n    \n# Create a VotingClassifier with various types of classifiers\nvoting_classifier = VotingClassifier(estimators=classifiers, voting='soft')  # Use 'hard' for hard voting\n\n# Train each individual classifier before training the ensemble\nfor name, classifier in classifiers:\n    classifier.fit(X_train, y_train)\n\n# Train the ensemble classifier\nvoting_classifier.fit(X_train, y_train)\n\n# Make predictions\ny_pred = voting_classifier.predict(X_test)\n\n# Evaluate accuracy for each classifier\naccuracies = {}\nfor name, classifier in classifiers:\n    y_pred_single = classifier.predict(X_test)\n    accuracy_single = accuracy_score(y_test, y_pred_single)\n    accuracies[name] = accuracy_single\n    print(f'{name} Accuracy: {accuracy_single:.2f}')\n\n# Evaluate overall ensemble accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Ensemble Accuracy: {accuracy:.2f}')\nfor name, classifier in classifiers:\n    classifier.fit(X_train, y_train)\n\n# Calculate weights based on accuracy\nclassifier_weights = calculate_weights(X_test, y_test, classifiers)\nvoting_classifier = VotingClassifier(estimators=classifiers, voting='soft', weights=list(classifier_weights.values()))\n\n# Entraînez le modèle sur l'ensemble d'apprentissage\nvoting_classifier.fit(X_train2, y_train2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#VOTING WITH VARIOUS CLASSIFIERS\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n# Assuming you have your data in X (features) and y (labels)\n\n\n\n\n\n# Chargez les données du dictionnaire\ndata_flat = [element for sous_liste in dict_label_bloc.values() for element in sous_liste]\ndata_flat=np.array(data_flat)\ndata_flat2=data_flat.reshape(320,-1)\n\nlab=np.array([[0]*32,[1]*32,[2]*32,[3]*32,[4]*32,[5]*32,[6]*32,[7]*32,[8]*32,[9]*32])\nlab = [element for sous_liste in lab for element in sous_liste]\n\nfor i in range(len(lab)):\n    lab[i]=np.array(lab[i])\n# Séparez les données en ensemble de test et d'apprentissage\n\nX_train2 = data_flat2\ny_train2 = lab\n#les data pour la classification du voting\n\n# Séparer les données en ensemble de test et d'apprentissage\nX_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=7)\n#les data pour les classifications dans le boosting pour avoir une accuracy\n\n#Classifiers :    \nclassifiers = [\n    ('rf', RandomForestClassifier(n_estimators=440, random_state=1)),\n    ('dt', DecisionTreeClassifier(random_state=5)),\n    ('lr', LogisticRegression(random_state=4)),\n    ('knn', KNeighborsClassifier(n_neighbors=2)),\n    ('extra_trees', ExtraTreesClassifier(n_estimators=100, random_state=9)),\n]\n#Creation of voting classifier :\n    \n# Create a VotingClassifier with various types of classifiers\nvoting_classifier = VotingClassifier(estimators=classifiers, voting='soft')  # Use 'hard' for hard voting\n\n# Train each individual classifier before training the ensemble\nfor name, classifier in classifiers:\n    classifier.fit(X_train, y_train)\n\n# Train the ensemble classifier\nvoting_classifier.fit(X_train, y_train)\n\n# Make predictions\ny_pred = voting_classifier.predict(X_test)\n\n# Evaluate accuracy for each classifier\naccuracies = {}\nfor name, classifier in classifiers:\n    y_pred_single = classifier.predict(X_test)\n    accuracy_single = accuracy_score(y_test, y_pred_single)\n    accuracies[name] = accuracy_single\n    print(f'{name} Accuracy: {accuracy_single:.2f}')\n\n# Evaluate overall ensemble accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Ensemble Accuracy: {accuracy:.2f}')\nfor name, classifier in classifiers:\n    classifier.fit(X_train, y_train)\n\n# Calculate weights based on accuracy\nclassifier_weights = calculate_weights(X_test, y_test, classifiers)\nvoting_classifier3 = VotingClassifier(estimators=classifiers, voting='soft', weights=list(classifier_weights.values()))\n\n# Entraînez le modèle sur l'ensemble d'apprentissage\nvoting_classifier3.fit(X_train2, y_train2)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:16:58.717582Z","iopub.execute_input":"2023-12-08T18:16:58.718493Z","iopub.status.idle":"2023-12-08T18:17:11.152492Z","shell.execute_reply.started":"2023-12-08T18:16:58.718434Z","shell.execute_reply":"2023-12-08T18:17:11.151527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_voting3=voting_classifier3.predict(data_flat2_test)\n\nimport pandas as pd\nimport numpy as np\n# Remplacez cela par vos prédictions réelles\ndf = pd.DataFrame(y_pred_voting3, columns=['label'])\n\n# Mapping des remplacements\nmapping = {\n    0: 'Ecole',\n    1: 'Joyeux',\n    2: 'Decider',\n    3: 'Carnaval',\n    4: 'Pyjama',\n    5: 'Ruisseau',\n    6: 'Addition',\n    7: 'Huitre',\n    8: 'Fillette',\n    9: 'Musique'\n}\n\n# Remplacement des valeurs dans le DataFrame\ndf.replace({'label': mapping}, inplace=True)\n\n# Ajouter une colonne 'id' contenant l'indice de ligne\ndf.insert(0, 'id', df.index)\n\n# Sauvegarder le DataFrame en tant que fichier CSV avec une première ligne \"id,label\"\ncsv_filename = \"submission.csv\"\ndf.to_csv(csv_filename, index=False)\n\n# Lecture du fichier CSV pour vérification\ntest_output = pd.read_csv(csv_filename)\n\n# Affichage du DataFrame\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:17:43.670662Z","iopub.execute_input":"2023-12-08T18:17:43.671595Z","iopub.status.idle":"2023-12-08T18:17:43.856969Z","shell.execute_reply.started":"2023-12-08T18:17:43.671540Z","shell.execute_reply":"2023-12-08T18:17:43.855954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"New attempt with non boosted random forests (3rd)","metadata":{}},{"cell_type":"code","source":"#RANDOM FOREST WITH XtremeTREE ON VOTING CLASSFIER FOR TEST DATA WITH ALL TRAINING DATA\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\n\n# Chargez les données du dictionnaire\ndata_flat = [element for sous_liste in dict_label_bloc.values() for element in sous_liste]\ndata_flat=np.array(data_flat)\ndata_flat2=data_flat.reshape(320,-1)\n\nlab=np.array([[0]*32,[1]*32,[2]*32,[3]*32,[4]*32,[5]*32,[6]*32,[7]*32,[8]*32,[9]*32])\nlab = [element for sous_liste in lab for element in sous_liste]\n\nfor i in range(len(lab)):\n    lab[i]=np.array(lab[i])\n# Séparez les données en ensemble de test et d'apprentissage\n\nX_train2 = data_flat2\ny_train2 = lab\n#les data pour la classification du voting\n\n# Séparer les données en ensemble de test et d'apprentissage\nX_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=7)\n#les data pour les classifications dans le boosting pour avoir une accuracy\n\n#Classifiers :    \nclassifiers = []\n\n# Assuming you want 10 sets of AdaBoost with RandomForest and 10 sets of ExtraTrees classifiers\nnum_classifiers = 5\n\nfor i in range(1, num_classifiers + 1):\n    rf_classifier = RandomForestClassifier(n_estimators=825, random_state=i)\n    extra_trees_classifier = ExtraTreesClassifier(n_estimators=800, random_state=i + 20)\n    classifiers.append(('rf_{}'.format(i), rf_classifier))\n    classifiers.append(('extra_trees_{}'.format(i), extra_trees_classifier))\n    \n#Creation of voting classifier :\n    \n# Create a VotingClassifier with various types of classifiers\nvoting_classifier = VotingClassifier(estimators=classifiers, voting='soft')  # Use 'hard' for hard voting\n\n# Train each individual classifier before training the ensemble\nfor name, classifier in classifiers:\n    classifier.fit(X_train, y_train)\n\n# Train the ensemble classifier\nvoting_classifier.fit(X_train, y_train)\n\n# Make predictions\ny_pred = voting_classifier.predict(X_test)\n\n# Evaluate accuracy for each classifier\naccuracies = {}\nfor name, classifier in classifiers:\n    y_pred_single = classifier.predict(X_test)\n    accuracy_single = accuracy_score(y_test, y_pred_single)\n    accuracies[name] = accuracy_single\n    print(f'{name} Accuracy: {accuracy_single:.2f}')\n\n# Evaluate overall ensemble accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Ensemble Accuracy: {accuracy:.2f}')\nfor name, classifier in classifiers:\n    classifier.fit(X_train, y_train)\n\n# Calculate weights based on accuracy\nclassifier_weights = calculate_weights(X_test, y_test, classifiers)\nvoting_classifier2 = VotingClassifier(estimators=classifiers, voting='soft', weights=list(classifier_weights.values()))\n\n# Entraînez le modèle sur l'ensemble d'apprentissage\nvoting_classifier2.fit(X_train2, y_train2)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T17:49:57.383039Z","iopub.execute_input":"2023-12-08T17:49:57.383417Z","iopub.status.idle":"2023-12-08T17:52:07.986315Z","shell.execute_reply.started":"2023-12-08T17:49:57.383383Z","shell.execute_reply":"2023-12-08T17:52:07.985281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Production of the third submission","metadata":{}},{"cell_type":"code","source":"y_pred_voting2=voting_classifier2.predict(data_flat2_test)\n\nimport pandas as pd\nimport numpy as np\n# Remplacez cela par vos prédictions réelles\ndf = pd.DataFrame(y_pred_voting2, columns=['label'])\n\n# Mapping des remplacements\nmapping = {\n    0: 'Ecole',\n    1: 'Joyeux',\n    2: 'Decider',\n    3: 'Carnaval',\n    4: 'Pyjama',\n    5: 'Ruisseau',\n    6: 'Addition',\n    7: 'Huitre',\n    8: 'Fillette',\n    9: 'Musique'\n}\n\n# Remplacement des valeurs dans le DataFrame\ndf.replace({'label': mapping}, inplace=True)\n\n# Ajouter une colonne 'id' contenant l'indice de ligne\ndf.insert(0, 'id', df.index)\n\n# Sauvegarder le DataFrame en tant que fichier CSV avec une première ligne \"id,label\"\ncsv_filename = \"submission.csv\"\ndf.to_csv(csv_filename, index=False)\n\n# Lecture du fichier CSV pour vérification\ntest_output = pd.read_csv(csv_filename)\n\n# Affichage du DataFrame\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T17:52:18.453680Z","iopub.execute_input":"2023-12-08T17:52:18.454739Z","iopub.status.idle":"2023-12-08T17:52:19.199007Z","shell.execute_reply.started":"2023-12-08T17:52:18.454698Z","shell.execute_reply":"2023-12-08T17:52:19.197953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"creation of the second attempt file","metadata":{}},{"cell_type":"code","source":"y_pred_voting2=voting_classifier2.predict(data_flat2_test)\n\nimport pandas as pd\nimport numpy as np\n# Remplacez cela par vos prédictions réelles\ndf = pd.DataFrame(y_pred_voting2, columns=['label'])\n\n# Mapping des remplacements\nmapping = {\n    0: 'Ecole',\n    1: 'Joyeux',\n    2: 'Decider',\n    3: 'Carnaval',\n    4: 'Pyjama',\n    5: 'Ruisseau',\n    6: 'Addition',\n    7: 'Huitre',\n    8: 'Fillette',\n    9: 'Musique'\n}\n\n# Remplacement des valeurs dans le DataFrame\ndf.replace({'label': mapping}, inplace=True)\n\n# Ajouter une colonne 'id' contenant l'indice de ligne\ndf.insert(0, 'id', df.index)\n\n# Sauvegarder le DataFrame en tant que fichier CSV avec une première ligne \"id,label\"\ncsv_filename = \"submission.csv\"\ndf.to_csv(csv_filename, index=False)\n\n# Lecture du fichier CSV pour vérification\ntest_output = pd.read_csv(csv_filename)\n\n# Affichage du DataFrame\nprint(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"classification for the first attempt (Random forest)","metadata":{}},{"cell_type":"code","source":"#RANDOM FOREST CLASSFIER FOR TEST DATA WITH ALL TRAINING DATA\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\n# Chargez les données du dictionnaire\ndata_flat = [element for sous_liste in dict_label_bloc.values() for element in sous_liste]\ndata_flat=np.array(data_flat)\ndata_flat2=data_flat.reshape(320,-1)\n\nlab=np.array([[0]*32,[1]*32,[2]*32,[3]*32,[4]*32,[5]*32,[6]*32,[7]*32,[8]*32,[9]*32])\nlab = [element for sous_liste in lab for element in sous_liste]\n\nfor i in range(len(lab)):\n    lab[i]=np.array(lab[i])\n# Séparez les données en ensemble de test et d'apprentissage\n\nX_train = data_flat2\ny_train = lab\n\n# Créez un modèle Random Forest fair varier le n__estimator\nclf = RandomForestClassifier(n_estimators=650)\n\n# Entraînez le modèle sur l'ensemble d'apprentissage\nclf.fit(X_train, y_train)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#BOOSTED RANDOM FOREST CLASSFIER FOR TEST DATA WITH ALL TRAINING DATA\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Chargez les données du dictionnaire\ndata_flat = [element for sous_liste in dict_label_bloc.values() for element in sous_liste]\ndata_flat=np.array(data_flat)\ndata_flat2=data_flat.reshape(320,-1)\n\nlab=np.array([[0]*32,[1]*32,[2]*32,[3]*32,[4]*32,[5]*32,[6]*32,[7]*32,[8]*32,[9]*32])\nlab = [element for sous_liste in lab for element in sous_liste]\n\nfor i in range(len(lab)):\n    lab[i]=np.array(lab[i])\n# Séparez les données en ensemble de test et d'apprentissage\n\nX_train = data_flat2\ny_train = lab\n\n# Créez un modèle Random Forest fair varier le n__estimator\nAB_rf = AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=150, random_state=4), n_estimators=10, random_state= 12)\n\n# Entraînez le modèle sur l'ensemble d'apprentissage\nAB_rf.fit(X_train, y_train)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:27:55.475530Z","iopub.execute_input":"2023-12-08T18:27:55.476580Z","iopub.status.idle":"2023-12-08T18:27:56.434103Z","shell.execute_reply.started":"2023-12-08T18:27:55.476538Z","shell.execute_reply":"2023-12-08T18:27:56.433124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"prediction 5","metadata":{}},{"cell_type":"code","source":"y_pred5 = AB_rf.predict(data_flat2_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:28:44.391950Z","iopub.execute_input":"2023-12-08T18:28:44.392358Z","iopub.status.idle":"2023-12-08T18:28:44.413745Z","shell.execute_reply.started":"2023-12-08T18:28:44.392319Z","shell.execute_reply":"2023-12-08T18:28:44.412918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creation of the prediction 1 ","metadata":{}},{"cell_type":"code","source":"y_pred = clf.predict(data_flat2_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"creation of the prediction 2","metadata":{}},{"cell_type":"code","source":"y_pred_voting = voting_classifier.predict(data_flat2_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Exportation of the first attempts","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n# Remplacez cela par vos prédictions réelles\ndf = pd.DataFrame(y_pred, columns=['label'])\n\n# Mapping des remplacements\nmapping = {\n    0: 'Ecole',\n    1: 'Joyeux',\n    2: 'Decider',\n    3: 'Carnaval',\n    4: 'Pyjama',\n    5: 'Ruisseau',\n    6: 'Addition',\n    7: 'Huitre',\n    8: 'Fillette',\n    9: 'Musique'\n}\n\n# Remplacement des valeurs dans le DataFrame\ndf.replace({'label': mapping}, inplace=True)\n\n# Ajouter une colonne 'id' contenant l'indice de ligne\ndf.insert(0, 'id', df.index)\n\n# Sauvegarder le DataFrame en tant que fichier CSV avec une première ligne \"id,label\"\ncsv_filename = \"submission.csv\"\ndf.to_csv(csv_filename, index=False)\n\n# Lecture du fichier CSV pour vérification\ntest_output = pd.read_csv(csv_filename)\n\n# Affichage du DataFrame\nprint(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"production du csv","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n# Remplacez cela par vos prédictions réelles\ndf = pd.DataFrame(y_pred5, columns=['label'])\n\n# Mapping des remplacements\nmapping = {\n    0: 'Ecole',\n    1: 'Joyeux',\n    2: 'Decider',\n    3: 'Carnaval',\n    4: 'Pyjama',\n    5: 'Ruisseau',\n    6: 'Addition',\n    7: 'Huitre',\n    8: 'Fillette',\n    9: 'Musique'\n}\n\n# Remplacement des valeurs dans le DataFrame\ndf.replace({'label': mapping}, inplace=True)\n\n# Ajouter une colonne 'id' contenant l'indice de ligne\ndf.insert(0, 'id', df.index)\n\n# Sauvegarder le DataFrame en tant que fichier CSV avec une première ligne \"id,label\"\ncsv_filename = \"submission.csv\"\ndf.to_csv(csv_filename, index=False)\n\n# Lecture du fichier CSV pour vérification\ntest_output = pd.read_csv(csv_filename)\n\n# Affichage du DataFrame\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:30:20.364604Z","iopub.execute_input":"2023-12-08T18:30:20.365438Z","iopub.status.idle":"2023-12-08T18:30:20.380983Z","shell.execute_reply.started":"2023-12-08T18:30:20.365403Z","shell.execute_reply":"2023-12-08T18:30:20.380031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Exportation of the second attempt","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n# Remplacez cela par vos prédictions réelles\ndf = pd.DataFrame(y_pred_voting, columns=['label'])\n\n# Mapping des remplacements\nmapping = {\n    0: 'Ecole',\n    1: 'Joyeux',\n    2: 'Decider',\n    3: 'Carnaval',\n    4: 'Pyjama',\n    5: 'Ruisseau',\n    6: 'Addition',\n    7: 'Huitre',\n    8: 'Fillette',\n    9: 'Musique'\n}\n\n# Remplacement des valeurs dans le DataFrame\ndf.replace({'label': mapping}, inplace=True)\n\n# Ajouter une colonne 'id' contenant l'indice de ligne\ndf.insert(0, 'id', df.index)\n\n# Sauvegarder le DataFrame en tant que fichier CSV avec une première ligne \"id,label\"\ncsv_filename = \"submission.csv\"\ndf.to_csv(csv_filename, index=False)\n\n# Lecture du fichier CSV pour vérification\ntest_output = pd.read_csv(csv_filename)\n\n# Affichage du DataFrame\nprint(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}